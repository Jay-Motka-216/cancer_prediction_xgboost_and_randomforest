{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer using Xgboost and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor #for the model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc #for model evaluation\n",
    "from sklearn.metrics import classification_report #for model evaluation\n",
    "from sklearn.metrics import confusion_matrix #for model evaluation\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "data = pd.read_csv(\"breast_cancer.csv\",index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                         \n",
       "842302           M        17.99         10.38          122.80     1001.0   \n",
       "842517           M        20.57         17.77          132.90     1326.0   \n",
       "84300903         M        19.69         21.25          130.00     1203.0   \n",
       "84348301         M        11.42         20.38           77.58      386.1   \n",
       "84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                            \n",
       "842302            0.11840           0.27760          0.3001   \n",
       "842517            0.08474           0.07864          0.0869   \n",
       "84300903          0.10960           0.15990          0.1974   \n",
       "84348301          0.14250           0.28390          0.2414   \n",
       "84358402          0.10030           0.13280          0.1980   \n",
       "\n",
       "          concave points_mean  symmetry_mean  ...  texture_worst  \\\n",
       "id                                            ...                  \n",
       "842302                0.14710         0.2419  ...          17.33   \n",
       "842517                0.07017         0.1812  ...          23.41   \n",
       "84300903              0.12790         0.2069  ...          25.53   \n",
       "84348301              0.10520         0.2597  ...          26.50   \n",
       "84358402              0.10430         0.1809  ...          16.67   \n",
       "\n",
       "          perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "id                                                                           \n",
       "842302             184.60      2019.0            0.1622             0.6656   \n",
       "842517             158.80      1956.0            0.1238             0.1866   \n",
       "84300903           152.50      1709.0            0.1444             0.4245   \n",
       "84348301            98.87       567.7            0.2098             0.8663   \n",
       "84358402           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "          concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "id                                                                \n",
       "842302             0.7119                0.2654          0.4601   \n",
       "842517             0.2416                0.1860          0.2750   \n",
       "84300903           0.4504                0.2430          0.3613   \n",
       "84348301           0.6869                0.2575          0.6638   \n",
       "84358402           0.4000                0.1625          0.2364   \n",
       "\n",
       "          fractal_dimension_worst  Unnamed: 32  \n",
       "id                                              \n",
       "842302                    0.11890          NaN  \n",
       "842517                    0.08902          NaN  \n",
       "84300903                  0.08758          NaN  \n",
       "84348301                  0.17300          NaN  \n",
       "84358402                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 569 entries, 842302 to 92751\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      " 31  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), object(1)\n",
      "memory usage: 146.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, subset=['diagnosis'], inplace=True)\n",
    "\n",
    "y = data.diagnosis\n",
    "data.drop(['diagnosis','Unnamed: 32'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "\n",
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique()<60 and X_train[cname].dtype==\"object\"]\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['radius_mean',\n",
       " 'texture_mean',\n",
       " 'perimeter_mean',\n",
       " 'area_mean',\n",
       " 'smoothness_mean',\n",
       " 'compactness_mean',\n",
       " 'concavity_mean',\n",
       " 'concave points_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean',\n",
       " 'radius_se',\n",
       " 'texture_se',\n",
       " 'perimeter_se',\n",
       " 'area_se',\n",
       " 'smoothness_se',\n",
       " 'compactness_se',\n",
       " 'concavity_se',\n",
       " 'concave points_se',\n",
       " 'symmetry_se',\n",
       " 'fractal_dimension_se',\n",
       " 'radius_worst',\n",
       " 'texture_worst',\n",
       " 'perimeter_worst',\n",
       " 'area_worst',\n",
       " 'smoothness_worst',\n",
       " 'compactness_worst',\n",
       " 'concavity_worst',\n",
       " 'concave points_worst',\n",
       " 'symmetry_worst',\n",
       " 'fractal_dimension_worst']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train.columns if \n",
    "                X_train[cname].dtype in ['int64', 'float64']]\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train_selected = X_train[my_cols].copy()\n",
    "X_test_selected = X_test[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RandomForestRegressor in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[64  3]\n",
      " [ 1 46]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.98      0.96      0.97        67\n",
      "           M       0.94      0.98      0.96        47\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.97      0.96       114\n",
      "weighted avg       0.97      0.96      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model) ])\n",
    "\n",
    "X_train_rf = X_train_selected.copy()\n",
    "X_test_rf = X_test_selected.copy()\n",
    "\n",
    "# Fit model into pipeline\n",
    "my_pipeline.fit(X_train_rf, y_train)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "y_pred = my_pipeline.predict(X_test_rf)\n",
    "# print(mean_squared_error(y_valid, val_predictions))\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForestRegressor with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 300, 500, 700, 900, 1100], 'max_features': ['auto', 'sqrt'], 'max_depth': [5, 10, 15, 20, 25, 30], 'min_samples_split': [2, 5, 10, 15, 100], 'min_samples_leaf': [1, 2, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1100, num = 6)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   29.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=1,\n",
       "                   param_distributions={'max_depth': [5, 10, 15, 20, 25, 30],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
       "                                        'min_samples_split': [2, 5, 10, 15,\n",
       "                                                              100],\n",
       "                                        'n_estimators': [100, 300, 500, 700,\n",
       "                                                         900, 1100]},\n",
       "                   random_state=0, verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestClassifier()\n",
    "\n",
    "# Data preprocessing pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# # Transform the data\n",
    "X_train_rf_r = X_train.copy()\n",
    "X_test_rf_r = X_test.copy()\n",
    "\n",
    "X_train_rf_r = my_pipeline.fit_transform(X_train_rf_r)\n",
    "X_test_rf_r = my_pipeline.transform(X_test_rf_r)\n",
    "\n",
    "\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = regr, param_distributions = random_grid, scoring='neg_mean_squared_error',\n",
    "#                               n_iter = 10, cv = 4, verbose= 1, random_state= 0, n_jobs = 1)\n",
    "rf_random = RandomizedSearchCV(estimator = regr, param_distributions = random_grid, \n",
    "                              n_iter = 10, cv = 4, verbose= 1, random_state= 0, n_jobs = 1)\n",
    "\n",
    "\n",
    "rf_random.fit(X_train_rf_r,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 900,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 30}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_y_pred = rf_random.predict(X_test_rf_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65  2]\n",
      " [ 2 45]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,rs_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.97      0.97      0.97        67\n",
      "           M       0.96      0.96      0.96        47\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.96      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,rs_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951715572116131"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Xgboost in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.96      0.99      0.97        67\n",
      "           M       0.98      0.94      0.96        47\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "[[66  1]\n",
      " [ 3 44]]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=1000, learning_rate=0.05) # Typical values range from 100-1000\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', xgb_model)\n",
    "                             ])\n",
    "\n",
    "\n",
    "# Fit model\n",
    "X_test_eval = X_test.copy()\n",
    "# Remove the model from pipeline\n",
    "eval_set_pipe = Pipeline(steps = [('preprocessor', preprocessor)])\n",
    "# fit transform X_valid.copy()\n",
    "X_test_eval = eval_set_pipe.fit(X_train, y_train).transform (X_test_eval)\n",
    "\n",
    "\n",
    "X_train_xg = X_train.copy()\n",
    "X_test_xg = X_test.copy()\n",
    "\n",
    "xgb_pipeline.fit(X_train_xg, y_train, model__early_stopping_rounds=5, model__eval_metric = \"mae\", \n",
    "                 model__eval_set=[(X_test_eval, y_test)],model__verbose=False)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "xgb_y_predictions = xgb_pipeline.predict(X_test_xg)\n",
    "#print(mean_squared_error(y_test, xgb_y_predictions))\n",
    "print(classification_report(y_test,xgb_y_predictions))\n",
    "print(confusion_matrix(y_test,xgb_y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Xgboost in pipeline using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.96      0.99      0.97        67\n",
      "           M       0.98      0.94      0.96        47\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "[[66  1]\n",
      " [ 3 44]]\n"
     ]
    }
   ],
   "source": [
    "xgb_pipeline_raw = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', xgb_model)\n",
    "                             ])\n",
    "\n",
    "X_train_xg = X_train.copy()\n",
    "X_test_xg = X_test.copy()\n",
    "\n",
    "xgb_pipeline_raw.fit(X_train_xg, y_train)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "xgb_y_predictions = xgb_pipeline_raw.predict(X_test_xg)\n",
    "# print(mean_squared_error(y_valid, xgb_val_predictions))\n",
    "print(classification_report(y_test,xgb_y_predictions))\n",
    "print(confusion_matrix(y_test,xgb_y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Xgboost with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.93      0.96      0.94        67\n",
      "           M       0.93      0.89      0.91        47\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.93      0.92      0.93       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n",
      "[[64  3]\n",
      " [ 5 42]]\n"
     ]
    }
   ],
   "source": [
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": range(100,1000,200),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "# xgbreg = XGBRegressor(nthreads=-1)  \n",
    "# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                               ('model', xgbreg)\n",
    "#                              ])\n",
    "\n",
    "# gs = RandomizedSearchCV(xgb_pipeline, params, n_jobs=1)\n",
    "# gs.fit(X_train, y_train,early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False) # cyin: has error\n",
    "# gs.best_model_\n",
    "\n",
    "#  cyin: xgboost using RandomizedSearchCV needs to seperate with pipeline otherwise error: \n",
    "#     use pipeline only for preprocessing \n",
    "\n",
    "# Define model\n",
    "xgbreg = XGBClassifier(n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# Data preprocessing pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Transform the data\n",
    "X_train_xg_r = X_train.copy()\n",
    "X_test_xg_r = X_test.copy()\n",
    "\n",
    "X_train_xg_r = my_pipeline.fit_transform(X_train_xg_r)\n",
    "X_test_xg_r = my_pipeline.transform(X_test_xg_r)\n",
    "\n",
    "\n",
    "# xgb_model = RandomizedSearchCV(xgbreg, params, scoring='neg_mean_squared_error', n_jobs=1)  \n",
    "xgb_model = RandomizedSearchCV(xgbreg, params, n_jobs=1)  \n",
    "\n",
    "xgb_model.fit(X_train_xg_r, y_train)  \n",
    "# rs_model.best_estimator_\n",
    "\n",
    "\n",
    "# get predicted prices on validation data\n",
    "xgb_y_predictions = xgb_model.predict(X_test_xg_r)\n",
    "# print(mean_squared_error(y_valid, xgb_val_predictions))\n",
    "print(classification_report(y_test,xgb_y_predictions))\n",
    "print(confusion_matrix(y_test,xgb_y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Summarry -  'Random Forest has best accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
